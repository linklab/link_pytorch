{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## torch.tensor vs. torch.Tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(1), tensor([0.]))"
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "i = torch.tensor(1)  # gives you a tensor with fixed value of 1\n",
    "j = torch.Tensor(1)  # gives you a tensor of size 1 which is randomly initialized\n",
    "i, j"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'float'>\n",
      "28\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "a = 45   # int64\n",
    "b = 45.2 # float64\n",
    "print(type(a))\n",
    "print(type(b))\n",
    "print(sys.getsizeof(a)) # prints 28 bytes\n",
    "print(sys.getsizeof(b)) # prints 24 bytes\n",
    "\n",
    "# https://stackoverflow.com/questions/31469553/python-memory-consumption-in-64-bit-system-for-int-and-float\n",
    "# 64 bit system\n",
    "# int:   overhead = 20 bytes, value = 8 bytes\n",
    "# float: overhead = 16 bytes, value = 8 bytes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "float64\n",
      "3\n",
      "3\n",
      "8\n",
      "8\n",
      "24\n",
      "24\n",
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_int = np.array([1, 2, 3])         # int64\n",
    "n_float = np.array([1.0, 2.0, 3.0]) # float64\n",
    "print(n_int.dtype)\n",
    "print(n_float.dtype)\n",
    "\n",
    "print(n_int.size)\n",
    "print(n_float.size)\n",
    "\n",
    "print(n_int.itemsize)\n",
    "print(n_float.itemsize)\n",
    "\n",
    "print(n_int.size * n_int.itemsize)\n",
    "print(n_float.size * n_float.itemsize)\n",
    "\n",
    "print(n_int.nbytes)\n",
    "print(n_float.nbytes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n",
      "torch.float64\n",
      "8\n",
      "4\n",
      "8\n",
      "3\n",
      "3\n",
      "3\n",
      "24\n",
      "12\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t_int = torch.tensor([1, 2, 3])                                 # int64\n",
    "t_float_1 = torch.tensor([1.0, 2.0, 3.0])                       # flaot32 !!!!!!\n",
    "t_float_2 = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)  # flaot64 !!!!!!\n",
    "print(t_int.dtype)\n",
    "print(t_float_1.dtype)\n",
    "print(t_float_2.dtype)\n",
    "\n",
    "print(t_int.element_size())\n",
    "print(t_float_1.element_size())\n",
    "print(t_float_2.element_size())\n",
    "\n",
    "print(t_int.nelement())\n",
    "print(t_float_1.nelement())\n",
    "print(t_float_2.nelement())\n",
    "\n",
    "print(t_int.element_size() * t_int.nelement())\n",
    "print(t_float_1.element_size() * t_float_1.nelement())\n",
    "print(t_float_2.element_size() * t_float_2.nelement())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## view vs. transpose vs. reshape\n",
    "- https://inmoonlight.github.io/2021/03/03/PyTorch-view-transpose-reshape/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 3, 2])"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[[0, 1],\n",
    "                   [2, 3],\n",
    "                   [4, 5]],\n",
    "                  [[6, 7],\n",
    "                   [8, 9],\n",
    "                   [10, 11]],\n",
    "                  [[12, 13],\n",
    "                   [14, 15],\n",
    "                   [16, 17]],\n",
    "                  [[18, 19],\n",
    "                   [20, 21],\n",
    "                   [22, 23]]])  # (4, 3, 2)\n",
    "t.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2],\n         [ 3,  4,  5]],\n\n        [[ 6,  7,  8],\n         [ 9, 10, 11]],\n\n        [[12, 13, 14],\n         [15, 16, 17]],\n\n        [[18, 19, 20],\n         [21, 22, 23]]])"
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv = t.view(4, 2, 3)\n",
    "tv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 2, 3])"
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.is_contiguous()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 3, 2])"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[[0, 1],\n",
    "                   [2, 3],\n",
    "                   [4, 5]],\n",
    "                  [[6, 7],\n",
    "                   [8, 9],\n",
    "                   [10, 11]],\n",
    "                  [[12, 13],\n",
    "                   [14, 15],\n",
    "                   [16, 17]],\n",
    "                  [[18, 19],\n",
    "                   [20, 21],\n",
    "                   [22, 23]]])  # (4, 3, 2)\n",
    "t.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### transpose\n",
    "- tensor 에 저장된 데이터의 물리적 위치 순서와 상관없이 수학적 의미의 transpose를 수행한다.\n",
    "- 따라서, 일반적으로 transpose가 수행된 tensor는 항상 contiguous 하지 않다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 2, 3])"
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = t.transpose(2, 1)\n",
    "tr.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  2,  4],\n         [ 1,  3,  5]],\n\n        [[ 6,  8, 10],\n         [ 7,  9, 11]],\n\n        [[12, 14, 16],\n         [13, 15, 17]],\n\n        [[18, 20, 22],\n         [19, 21, 23]]])"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.is_contiguous()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### view 는 contiguous 하지 않은 tensor에 대해서는 적용할 수 없다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[226], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "tr.view(4, 3, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  2,  4],\n         [ 1,  3,  5]],\n\n        [[ 6,  8, 10],\n         [ 7,  9, 11]],\n\n        [[12, 14, 16],\n         [13, 15, 17]],\n\n        [[18, 20, 22],\n         [19, 21, 23]]])"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr2 = tr.contiguous()\n",
    "tr2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  2],\n         [ 4,  1],\n         [ 3,  5]],\n\n        [[ 6,  8],\n         [10,  7],\n         [ 9, 11]],\n\n        [[12, 14],\n         [16, 13],\n         [15, 17]],\n\n        [[18, 20],\n         [22, 19],\n         [21, 23]]])"
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr3 = tr2.view(4, 3, 2)\n",
    "tr3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### tensor.reshape() == tensor.contiguous().view()\n",
    "- reshape 은 강제로 tensor를 contiguous 하게 만들면서 shape을 변경"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[230], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "tr4 = tr.view(4, 3, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  2],\n         [ 4,  1],\n         [ 3,  5]],\n\n        [[ 6,  8],\n         [10,  7],\n         [ 9, 11]],\n\n        [[12, 14],\n         [16, 13],\n         [15, 17]],\n\n        [[18, 20],\n         [22, 19],\n         [21, 23]]])"
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr4 = tr.reshape(4, 3, 2)\n",
    "tr4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Memory: List vs. Tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![...](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2020-1-21-pytorch_tensor/figure2.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4393155056 4396643056 4396634992\n"
     ]
    }
   ],
   "source": [
    "a0 = 1.0\n",
    "a1 = 2.0\n",
    "a2 = 3.0\n",
    "\n",
    "print(id(a0) , id(a1), id(a2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4393155056 4396643056 4396634992\n",
      "4397599744\n"
     ]
    }
   ],
   "source": [
    "a = [a0, a1, a2]\n",
    "\n",
    "print(id(a[0]) , id(a[1]), id(a[2]))\n",
    "print(id(a))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4767668704 4767668704 4767668704\n",
      "4498415088\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "b = torch.tensor([a0, a1, a2])\n",
    "\n",
    "print(id(b[0]), id(b[1]), id(b[2]))\n",
    "print(id(b))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Efficient Memory Management"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1.0, 2.0],\n",
    "        [3.0, 4.0],\n",
    "        [5.0, 6.0]]\n",
    "a = torch.tensor(data)\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4.])\n"
     ]
    }
   ],
   "source": [
    "b = a[1,:]\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-999.,    4.])\n",
      "tensor([[   1.,    2.],\n",
      "        [-999.,    4.],\n",
      "        [   5.,    6.]])\n"
     ]
    }
   ],
   "source": [
    "b[0] = -999\n",
    "print(b)\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1.0, 2.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[1.0, 2.0, 3.0]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2] = 3.0\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 1., 1.])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch # <1>\n",
    "a = torch.ones(3) # <2>\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 1., 2.])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2] = 2.0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.zeros(6) # <1>\n",
    "points[0] = 4.0 # <2>\n",
    "points[1] = 1.0\n",
    "points[2] = 5.0\n",
    "points[3] = 3.0\n",
    "points[4] = 2.0\n",
    "points[5] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([4., 1., 5., 3., 2., 1.])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(4.0, 1.0)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(points[0]), float(points[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 1.],\n        [5., 3.],\n        [2., 1.]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 2])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.zeros(3, 2)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 1.],\n        [5., 3.],\n        [2., 1.]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([4., 1.])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Storage, Offset, Stride\n",
    "- https://hiddenbeginner.github.io/deeplearning/2020/01/21/pytorch_tensor.html\n",
    "- 1차원 텐서이든, 2차원 텐서이든 혹은 d차원 텐서이든 모든 텐서는 메모리에 1차원 배열의 형태로 저장됨\n",
    "- storage: 텐서를 효율적으로 다룰 수 있도록 만들어주는 이러한 1차원 배열 (인덱싱한 텐서의 값에 접근할 때 offset과 stride 사용)\n",
    "![...](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2020-1-21-pytorch_tensor/figure3.png)\n",
    "  - 위 두 텐서에 대하여 storage의 offset은 모두 0\n",
    "  - stride는 텐서 안의 어떤 한 값에서 다음 원소를 얻기 위하여 storage에서 뛰어 넘어야할 인덱스의 개수\n",
    "    - storage의 stride는 텐서의 차원만큼 정의됨"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============storage==========\n",
      " 3\n",
      " 1\n",
      " 8\n",
      " 0\n",
      " 9\n",
      " 2\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]\n",
      "=============================\n",
      "Offset:  0\n",
      "Strides:  (3, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/800149035.py:4: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(a.storage())\n",
      "/Users/yhhan/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/storage.py:636: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f'device={self.device}) of size {len(self)}]')\n",
      "/Users/yhhan/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/storage.py:637: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if self.device.type == 'meta':\n",
      "/Users/yhhan/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/storage.py:640: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  data_str = ' ' + '\\n '.join(str(self[i]) for i in range(self.size()))\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[3,1,8],\n",
    "                  [0,9,2]])\n",
    "print(\"============storage==========\")\n",
    "print(a.storage())\n",
    "print(\"=============================\")\n",
    "print(\"Offset: \", a.storage_offset())\n",
    "print(\"Strides: \", a.stride())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/3297796694.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  points.storage()\n",
      "/Users/yhhan/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/IPython/lib/pretty.py:778: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  output = repr(obj)\n",
      "/Users/yhhan/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/storage.py:645: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return str(self)\n"
     ]
    },
    {
     "data": {
      "text/plain": " 4.0\n 1.0\n 5.0\n 3.0\n 2.0\n 1.0\n[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tenset[i, j] == storage[storage_offset + i * stirde[0] + j * stirde[1]]\n",
    "### tenset[i, j, k] == storage[storage_offset + i * stirde[0] + j * stirde[1] + k * stirde[2]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8],\n        [ 9, 10, 11]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.arange(12).view((4, 3))\n",
    "x1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 1)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.stride()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = x1.view(3, 4)\n",
    "x2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/2939581920.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = x2.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0, (4, 1))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage = x2.storage()\n",
    "offset = x2.storage_offset()\n",
    "stride = x2.stride()\n",
    "offset, stride"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "9"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item1 = x2[2, 1].item()\n",
    "item1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/250354989.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  item2 = storage[offset + 2 * x2.stride()[0] + 1 * x2.stride()[1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "9"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item2 = storage[offset + 2 * x2.stride()[0] + 1 * x2.stride()[1]]\n",
    "item2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/894859101.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = x.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0, (120, 4, 1))"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1200).view((10, 30, 4))\n",
    "storage = x.storage()\n",
    "offset = x.storage_offset()\n",
    "stride = x.stride()\n",
    "offset, stride"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/3116659906.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  item2 = storage[offset + 4 * stride[0] + 23 * stride[1] + 2 * stride[2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item1 = x[4, 23, 2].item()\n",
    "item2 = storage[offset + 4 * stride[0] + 23 * stride[1] + 2 * stride[2]]\n",
    "item1 == item2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![...](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2020-1-21-pytorch_tensor/figure4.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============storage==========\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 2\n",
      " 8\n",
      " 0\n",
      " 3\n",
      " 4\n",
      " 9\n",
      " 2\n",
      " 5\n",
      " 6\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 12]\n",
      "=============================\n",
      "Offset:  0\n",
      "Strides:  (4, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/822766986.py:5: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(a.storage())\n",
      "/Users/yhhan/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/storage.py:636: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f'device={self.device}) of size {len(self)}]')\n",
      "/Users/yhhan/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/storage.py:637: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if self.device.type == 'meta':\n",
      "/Users/yhhan/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/storage.py:640: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  data_str = ' ' + '\\n '.join(str(self[i]) for i in range(self.size()))\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[3,1,1,2],\n",
    "                  [8,0,3,4],\n",
    "                  [9,2,5,6]])\n",
    "print(\"============storage==========\")\n",
    "print(a.storage())\n",
    "print(\"=============================\")\n",
    "print(\"Offset: \", a.storage_offset())\n",
    "print(\"Strides: \", a.stride())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============storage==========\n",
      " 3\n",
      " 1\n",
      " 1\n",
      " 2\n",
      " 8\n",
      " 0\n",
      " 3\n",
      " 4\n",
      " 9\n",
      " 2\n",
      " 5\n",
      " 6\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 12]\n",
      "=============================\n",
      "Offset:  5\n",
      "Strides:  (4, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/692778318.py:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(b.storage())\n",
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/692778318.py:7: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = b.storage()\n"
     ]
    }
   ],
   "source": [
    "b = a[1:3, 1:3]\n",
    "print(\"============storage==========\")\n",
    "print(b.storage())\n",
    "print(\"=============================\")\n",
    "print(\"Offset: \", b.storage_offset())\n",
    "print(\"Strides: \", b.stride())\n",
    "storage = b.storage()\n",
    "offset = b.storage_offset()\n",
    "stride = b.stride()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/1294772978.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  id(a.storage()) == id(b.storage())\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a.storage()) == id(b.storage())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a.untyped_storage()) == id(b.untyped_storage())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 3],\n        [2, 5]])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item1 = b[1, 1].item()\n",
    "item1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/2310448803.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  item2 = storage[offset + 1 * stride[0] + 1 * stride[1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item2 = storage[offset + 1 * stride[0] + 1 * stride[1]]\n",
    "item1 == item2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/1412174013.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  points_storage = points.storage()\n",
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/1412174013.py:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  points_storage[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "4.0"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points_storage = points.storage()\n",
    "points_storage[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/682517886.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  points.storage()[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.storage()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/3332447618.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  points_storage = points.storage()\n",
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/3332447618.py:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  points_storage[0] = 2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[2., 1.],\n        [5., 3.],\n        [2., 1.]])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points_storage = points.storage()\n",
    "points_storage[0] = 2.0\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2, 1)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([5., 3.])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point = points[1]\n",
    "second_point"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.storage_offset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1,)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "3.0"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item1 = points[1, 1].item()\n",
    "item1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "3.0"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item2 = second_point[1].item()\n",
    "item2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/91671260.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  item3 = points_storage[second_point.storage_offset() + 1 * second_point.stride()[0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "3.0"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item3 = points_storage[second_point.storage_offset() + 1 * second_point.stride()[0]]\n",
    "item3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 4.,  1.],\n        [10.,  3.],\n        [ 2.,  1.]])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1]\n",
    "second_point[0] = 10.0\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 1.],\n        [5., 3.],\n        [2., 1.]])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1].clone()\n",
    "second_point[0] = 10.0\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 1.],\n        [5., 3.],\n        [2., 1.]])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 5., 2.],\n        [1., 3., 1.]])"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t = points.t()\n",
    "points_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/2277798614.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  id(points.storage()) == id(points_t.storage())\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(points.storage()) == id(points_t.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(points.untyped_storage()) == id(points_t.untyped_storage())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2, 1)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 2)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 4, 5])"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_t = torch.ones(3, 4, 5)\n",
    "some_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 4, 3])"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_t = some_t.transpose(0, 2)\n",
    "transpose_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(20, 5, 1)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 5, 20)"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 5., 2.],\n        [1., 3., 1.]])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points_t = points.t()\n",
    "points_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/2245626776.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  points_t.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": " 4.0\n 1.0\n 5.0\n 3.0\n 2.0\n 1.0\n[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 2)"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 5., 2.],\n        [1., 3., 1.]])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t_cont = points_t.contiguous()\n",
    "points_t_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 1)"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t_cont.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/hpl6nvhn3zzfqzdm_l66mdyc0000gn/T/ipykernel_43730/1075489549.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  points_t_cont.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": " 4.0\n 5.0\n 2.0\n 1.0\n 3.0\n 1.0\n[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t_cont.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_points = torch.ones(10, 2, dtype=torch.double)\n",
    "short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.int16"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_points.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_points = torch.zeros(10, 2).double()\n",
    "short_points = torch.ones(10, 2).short()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_points = torch.zeros(10, 2).to(torch.double)\n",
    "short_points = torch.ones(10, 2).to(dtype=torch.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64 = torch.rand(5, dtype=torch.double)  # <1>\n",
    "points_short = points_64.to(torch.short)\n",
    "points_64 * points_short  # works from PyTorch 1.3 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1, 2, 3, 4, 5]"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list = list(range(6))\n",
    "some_list[:]     # <1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3]"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[1:4]   # <2>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3, 4, 5]"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[1:]    # <3>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1, 2, 3]"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[:4]    # <4>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1, 2, 3, 4]"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[:-1]   # <5>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 3]"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[1:4:2] # <6>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[5., 3.],\n        [2., 1.]])"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset points back to original value\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points[1:]       # <1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[5., 3.],\n        [2., 1.]])"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, :]    # <2>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([5., 2.])"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, 0]    # <3>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[4., 1.],\n         [5., 3.],\n         [2., 1.]]])"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[None]     # <4>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with NumPy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]], dtype=float32)"
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.ones(3, 4)\n",
    "points_np = points.numpy()\n",
    "points_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(points) == id(points_np)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- points & points_np are objects of different Python types, so, as far as Python is concerned, they have different addresses\n",
    "- However, they share an underlying C array in memory & modifying one in-place also modies the other"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])"
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.zero_()\n",
    "points"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(points_np)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "points2 = torch.from_numpy(points_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[10., 10., 10., 10.],\n        [10., 10., 10., 10.],\n        [10., 10., 10., 10.]])"
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points2.add_(10)\n",
    "points2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[10., 10., 10., 10.],\n       [10., 10., 10., 10.],\n       [10., 10., 10., 10.]], dtype=float32)"
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[10., 10., 10., 10.],\n        [10., 10., 10., 10.],\n        [10., 10., 10., 10.]])"
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## torch save & load"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(points, '../data/p1ch3/ourpoints.t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/p1ch3/ourpoints.t','wb') as f:\n",
    "   torch.save(points, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.load('../data/p1ch3/ourpoints.t')\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 1.],\n        [5., 3.],\n        [2., 1.]])"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "with open('../data/p1ch3/ourpoints.t','rb') as f:\n",
    "   points = torch.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to truncate a file which is already open)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[159], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#  pip install h5py\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mh5py\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[43mh5py\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../data/p1ch3/ourpoints.hdf5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mw\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m dset \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mcreate_dataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcoords\u001B[39m\u001B[38;5;124m'\u001B[39m, data\u001B[38;5;241m=\u001B[39mpoints\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[1;32m      6\u001B[0m f\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/h5py/_hl/files.py:567\u001B[0m, in \u001B[0;36mFile.__init__\u001B[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001B[0m\n\u001B[1;32m    558\u001B[0m     fapl \u001B[38;5;241m=\u001B[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001B[1;32m    559\u001B[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001B[1;32m    560\u001B[0m                      alignment_threshold\u001B[38;5;241m=\u001B[39malignment_threshold,\n\u001B[1;32m    561\u001B[0m                      alignment_interval\u001B[38;5;241m=\u001B[39malignment_interval,\n\u001B[1;32m    562\u001B[0m                      meta_block_size\u001B[38;5;241m=\u001B[39mmeta_block_size,\n\u001B[1;32m    563\u001B[0m                      \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    564\u001B[0m     fcpl \u001B[38;5;241m=\u001B[39m make_fcpl(track_order\u001B[38;5;241m=\u001B[39mtrack_order, fs_strategy\u001B[38;5;241m=\u001B[39mfs_strategy,\n\u001B[1;32m    565\u001B[0m                      fs_persist\u001B[38;5;241m=\u001B[39mfs_persist, fs_threshold\u001B[38;5;241m=\u001B[39mfs_threshold,\n\u001B[1;32m    566\u001B[0m                      fs_page_size\u001B[38;5;241m=\u001B[39mfs_page_size)\n\u001B[0;32m--> 567\u001B[0m     fid \u001B[38;5;241m=\u001B[39m \u001B[43mmake_fid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muserblock_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfapl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfcpl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswmr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswmr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    569\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(libver, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    570\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_libver \u001B[38;5;241m=\u001B[39m libver\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/h5py/_hl/files.py:237\u001B[0m, in \u001B[0;36mmake_fid\u001B[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001B[0m\n\u001B[1;32m    235\u001B[0m     fid \u001B[38;5;241m=\u001B[39m h5f\u001B[38;5;241m.\u001B[39mcreate(name, h5f\u001B[38;5;241m.\u001B[39mACC_EXCL, fapl\u001B[38;5;241m=\u001B[39mfapl, fcpl\u001B[38;5;241m=\u001B[39mfcpl)\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 237\u001B[0m     fid \u001B[38;5;241m=\u001B[39m \u001B[43mh5f\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh5f\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mACC_TRUNC\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfapl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfapl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfcpl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfcpl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;66;03m# Open in append mode (read/write).\u001B[39;00m\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;66;03m# If that fails, create a new file only if it won't clobber an\u001B[39;00m\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;66;03m# existing one (ACC_EXCL)\u001B[39;00m\n\u001B[1;32m    242\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32mh5py/_objects.pyx:54\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mh5py/_objects.pyx:55\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mh5py/h5f.pyx:126\u001B[0m, in \u001B[0;36mh5py.h5f.create\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: Unable to create file (unable to truncate a file which is already open)"
     ]
    }
   ],
   "source": [
    "#  pip install h5py\n",
    "import h5py\n",
    "\n",
    "f = h5py.File('../data/p1ch3/ourpoints.hdf5', 'w')\n",
    "dset = f.create_dataset('coords', data=points.numpy())\n",
    "f.close()\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<HDF5 dataset \"coords\": shape (3, 2), type \"<f4\">"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File('../data/p1ch3/ourpoints.hdf5', 'r')\n",
    "dset = f['coords']\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_points = torch.from_numpy(dset[1:])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[5., 3.],\n        [2., 1.]])"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_points"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[163], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m points_gpu \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m5.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3.0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[164], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m points_gpu \u001B[38;5;241m=\u001B[39m \u001B[43mpoints\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "points_gpu = points.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[165], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m points_gpu \u001B[38;5;241m=\u001B[39m \u001B[43mpoints\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda:0\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "points_gpu = points.to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[166], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m points \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m points  \u001B[38;5;66;03m# <1>\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m points_gpu \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[43mpoints\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# <2>\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "points = 2 * points  # <1>\n",
    "points_gpu = 2 * points.to(device='cuda')  # <2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'points_gpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[167], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m points_gpu \u001B[38;5;241m=\u001B[39m \u001B[43mpoints_gpu\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m4\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'points_gpu' is not defined"
     ]
    }
   ],
   "source": [
    "points_gpu = points_gpu + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'points_gpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[168], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m points_cpu \u001B[38;5;241m=\u001B[39m \u001B[43mpoints_gpu\u001B[49m\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'points_gpu' is not defined"
     ]
    }
   ],
   "source": [
    "points_cpu = points_gpu.to(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[169], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m points_gpu \u001B[38;5;241m=\u001B[39m \u001B[43mpoints\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# <1>\u001B[39;00m\n\u001B[1;32m      2\u001B[0m points_gpu \u001B[38;5;241m=\u001B[39m points\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      3\u001B[0m points_cpu \u001B[38;5;241m=\u001B[39m points_gpu\u001B[38;5;241m.\u001B[39mcpu()\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "points_gpu = points.cuda()  # <1>\n",
    "points_gpu = points.cuda(0)\n",
    "points_cpu = points_gpu.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3, 2]), torch.Size([2, 3]))"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = torch.transpose(a, 0, 1)\n",
    "\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3, 2]), torch.Size([2, 3]))"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = a.transpose(0, 1)\n",
    "\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.zero_()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## torch.dot vs. torch.matmul vs. torch.mm vs. torch.bmm\n",
    "- https://velog.io/@regista/torch.dot-torch.matmul-torch.mm-torch.bmm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 3, 20, 30])"
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1 = torch.randn(10, 3, 20, 10)\n",
    "batch2 = torch.randn(10, 3, 10, 30)\n",
    "res = torch.matmul(batch1, batch2)\n",
    "res.size() # torch.Size([10, 3, 20, 30])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 3, 20, 30])"
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1 = torch.randn(10, 3, 20, 10)\n",
    "batch2 = torch.randn(10, 3, 10, 30)\n",
    "res = batch1 @ batch2\n",
    "res.size() # torch.Size([10, 3, 20, 30])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 20, 30])"
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1 = torch.randn(10, 20, 10)\n",
    "batch2 = torch.randn(10, 10, 30)\n",
    "res = torch.bmm(batch1, batch2)\n",
    "res.size() # torch.Size([10, 3, 20, 30])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
    "- mat1 and mat2 must be 3-D tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
