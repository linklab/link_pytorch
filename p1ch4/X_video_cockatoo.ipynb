{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, threshold=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to the shape of tensors, video data can be seen as equivalent to volumetric data, with `depth` replaced by the `time` dimension. The result is again a 5D tensor with shape `N x C x T x H x W`.\n",
    "\n",
    "There are several formats for video, especially geared towards compression by exploiting redundancies in space and time. Luckily for us, `imageio` reads video data as well. Suppose we'd like to retain 100 consecutive frames in our 512 x 512 RBG video for classifying an action using a convolutional neural network. We first create a reader instance for the video, that will allow us to get information about the video and iterate over the frames in time.\n",
    "Let's see what the meta data for the video looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No ffmpeg exe could be found. Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimageio\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m reader \u001B[38;5;241m=\u001B[39m \u001B[43mimageio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_reader\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../data/p1ch4/video-cockatoo/cockatoo.mp4\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m meta \u001B[38;5;241m=\u001B[39m reader\u001B[38;5;241m.\u001B[39mget_meta_data()\n\u001B[1;32m      5\u001B[0m meta\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/imageio/v2.py:160\u001B[0m, in \u001B[0;36mget_reader\u001B[0;34m(uri, format, mode, **kwargs)\u001B[0m\n\u001B[1;32m    157\u001B[0m imopen_args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlegacy_mode\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    159\u001B[0m image_file \u001B[38;5;241m=\u001B[39m imopen(uri, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m mode, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mimopen_args)\n\u001B[0;32m--> 160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimage_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlegacy_get_reader\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/imageio/core/legacy_plugin_wrapper.py:116\u001B[0m, in \u001B[0;36mLegacyPlugin.legacy_get_reader\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format\u001B[38;5;241m.\u001B[39mget_reader(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request)\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request\u001B[38;5;241m.\u001B[39mget_file()\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m--> 116\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_format\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_reader\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/imageio/core/format.py:221\u001B[0m, in \u001B[0;36mFormat.get_reader\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m select_mode \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodes:\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFormat \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m cannot read in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequest\u001B[38;5;241m.\u001B[39mmode\u001B[38;5;241m.\u001B[39mimage_mode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m mode\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    220\u001B[0m     )\n\u001B[0;32m--> 221\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mReader\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/imageio/core/format.py:312\u001B[0m, in \u001B[0;36mFormat._BaseReaderWriter.__init__\u001B[0;34m(self, format, request)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request \u001B[38;5;241m=\u001B[39m request\n\u001B[1;32m    311\u001B[0m \u001B[38;5;66;03m# Open the reader/writer\u001B[39;00m\n\u001B[0;32m--> 312\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/imageio/plugins/ffmpeg.py:343\u001B[0m, in \u001B[0;36mFfmpegFormat.Reader._open\u001B[0;34m(self, loop, size, dtype, pixelformat, print_info, ffmpeg_params, input_params, output_params, fps)\u001B[0m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;66;03m# Start ffmpeg subprocess and get meta information\u001B[39;00m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 343\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_initialize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m:\n\u001B[1;32m    345\u001B[0m     \u001B[38;5;66;03m# Specify input framerate again, this time different.\u001B[39;00m\n\u001B[1;32m    346\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m need_input_fps:\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/imageio/plugins/ffmpeg.py:494\u001B[0m, in \u001B[0;36mFfmpegFormat.Reader._initialize\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    492\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_meta\u001B[38;5;241m.\u001B[39mupdate(meta)\n\u001B[1;32m    493\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m index \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 494\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_meta\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_gen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__next__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    495\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    496\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_gen\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__next__\u001B[39m()\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/imageio_ffmpeg/_io.py:263\u001B[0m, in \u001B[0;36mread_frames\u001B[0;34m(path, pix_fmt, bpp, input_params, output_params, bits_per_pixel)\u001B[0m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;66;03m# ----- Prepare\u001B[39;00m\n\u001B[1;32m    261\u001B[0m pre_output_params \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-pix_fmt\u001B[39m\u001B[38;5;124m\"\u001B[39m, pix_fmt, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-vcodec\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrawvideo\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-f\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage2pipe\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m--> 263\u001B[0m cmd \u001B[38;5;241m=\u001B[39m [\u001B[43mget_ffmpeg_exe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[1;32m    264\u001B[0m cmd \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m input_params \u001B[38;5;241m+\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-i\u001B[39m\u001B[38;5;124m\"\u001B[39m, path]\n\u001B[1;32m    265\u001B[0m cmd \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m pre_output_params \u001B[38;5;241m+\u001B[39m output_params \u001B[38;5;241m+\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/miniforge3/envs/link_pytorch/lib/python3.10/site-packages/imageio_ffmpeg/_utils.py:33\u001B[0m, in \u001B[0;36mget_ffmpeg_exe\u001B[0;34m()\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m exe\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Nothing was found\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo ffmpeg exe could be found. Install ffmpeg on your system, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor set the IMAGEIO_FFMPEG_EXE environment variable.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     36\u001B[0m )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: No ffmpeg exe could be found. Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable."
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "reader = imageio.get_reader('../data/p1ch4/video-cockatoo/cockatoo.mp4')\n",
    "meta = reader.get_meta_data()\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the information to size the tensor that will store the video frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m n_channels \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m\n\u001B[0;32m----> 2\u001B[0m n_frames \u001B[38;5;241m=\u001B[39m \u001B[43mmeta\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnframes\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      3\u001B[0m video \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(n_channels, n_frames, \u001B[38;5;241m*\u001B[39mmeta[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msize\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      5\u001B[0m video\u001B[38;5;241m.\u001B[39mshape\n",
      "\u001B[0;31mNameError\u001B[0m: name 'meta' is not defined"
     ]
    }
   ],
   "source": [
    "n_channels = 3\n",
    "n_frames = meta['nframes']\n",
    "video = torch.empty(n_channels, n_frames, *meta['size'])\n",
    "\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just iterate over the reader and set the values for all three channels into in the proper `i`-th time slice.\n",
    "This might take a few seconds to finish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, frame_arr \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[43mreader\u001B[49m):\n\u001B[1;32m      2\u001B[0m     frame \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(frame_arr)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m      3\u001B[0m     video[:, i] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtranspose(frame, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'reader' is not defined"
     ]
    }
   ],
   "source": [
    "for i, frame_arr in enumerate(reader):\n",
    "    frame = torch.from_numpy(frame_arr).float()\n",
    "    video[:, i] = torch.transpose(frame, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we iterate over individual frames and set each frame in the `C x T x H x W` video tensor, after transposing the channel. We can then obtain a batch by stacking multiple 4D tensors or pre-allocating a 5D tensor with a known batch size and filling it iteratively, clip by clip, assuming clips are trimmed to a fixed number of frames.\n",
    "\n",
    "Equating video data to volumetric data is not the only way to represent video for training purposes. This is a valid strategy if we deal with video bursts of fixed length. An alternative strategy is to resort to network architectures capable of processing long sequences and exploiting short and long-term relationships in time, just like for text or audio.\n",
    "// We'll see this kind of architectures when we take on recurrent networks.\n",
    "\n",
    "This next approach accounts for time along the batch dimension. Hence, we'll build our dataset as a 4D tensor, stacking frame by frame in the batch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m time_video \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(\u001B[43mn_frames\u001B[49m, n_channels, \u001B[38;5;241m*\u001B[39mmeta[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msize\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, frame \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(reader):\n\u001B[1;32m      4\u001B[0m     frame \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(frame)\u001B[38;5;241m.\u001B[39mfloat()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'n_frames' is not defined"
     ]
    }
   ],
   "source": [
    "time_video = torch.empty(n_frames, n_channels, *meta['size'])\n",
    "\n",
    "for i, frame in enumerate(reader):\n",
    "    frame = torch.from_numpy(frame).float()\n",
    "    time_video[i] = torch.transpose(frame, 0, 2)\n",
    "\n",
    "time_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
